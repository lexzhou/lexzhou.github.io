---
layout: about
title: About
permalink: /
subtitle: 

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  # more_info: >
  #  <p>555 your office number</p>
  #  <p>123 your address street</p>
  #  <p>Your City, State 12345</p>

news: true # includes a list of news items
latest_posts: false # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Hi! I am a 1st-year CS PhD candidate at Princeton University, advised Prof. [Peter Henderson](https://www.peterhenderson.co/) at the [POLARIS Lab](https://www.polarislab.org/). Before joining Princeton, I was a StarBridge Scholar at Microsoft Research for a year, working with Dr. [Xing Xie][xxie].  I did my master's in CS at the University of Cambridge, funded by [Open Philanthropy][op] and supervised by Prof. [Andreas Vlachos][avlachos], and my BSc in Data Science at the Universitat Politècnica de València, where I got into research by working with Prof. [Jose Hernandez-Orallo][jhorallo].

Over the past few years, my research interests have centered on evaluating AI capabilities, societal impact, and safety, with a special emphasis on general-purpose AI systems and agents. I regularly draw inspirations from interdisciplinary fields beyond CS, such as psychometrics and cognitive science. At present, I mostly spend my days designing evaluation frameworks that are not only robust, valid, efficient, and hard to game, but also capable of *causally explaining and predicting* AI’s capabilities and limitations across a wide range of scenarios—rather than being constrained to often arbitrary sets of narrow, task-specific benchmarks that lack explanatory and predictive power. I also spend a large share of my time to both assess and anticipate societal impacts associated with AI, in the quest of minimizing AI harms while amplifying their benefits for humans.

I've spent time in research/consultancy roles at Microsoft Research, OpenAI, Meta AI, European Commission JRC, Krueger AI Safety Lab, and VRAIN. My work has been featured in [Nature](https://www.nature.com/articles/d41586-024-03137-3), [Financial Times](https://www.ft.com/content/0876687a-f8b7-4b39-b513-5fee942831e8), [Microsoft Research](https://www.microsoft.com/en-us/research/blog/predicting-and-explaining-ai-model-performance-a-new-approach-to-evaluation/), [MIT Tech Review](https://mp.weixin.qq.com/s/T2aqVlWePuRfEEuIP5_yqg), [Forbes](https://www.forbes.com/sites/delltechnologies/2024/10/29/steer-your-ai-strategy-straight-amid-the-jagged-frontier/), [IEEE Spectrum](https://spectrum.ieee.org/chatgpt-reliability), [El País](https://english.elpais.com/technology/2024-09-25/new-ai-models-like-chatgpt-pursue-superintelligence-but-cant-be-trusted-even-when-it-comes-to-basic-questions.html), [New Scientists](https://www.newscientist.com/article/2449427-ais-get-worse-at-answering-simple-questions-as-they-get-bigger/), [QbitAI](https://mp.weixin.qq.com/s/VCvkSUdKT7ZgBaeLWKVoTg), [IBM](https://www.ibm.com/blog/llms-and-reliability/), among others.

If you are drawn to everything relevant to AI Evaluation and wanna stay informed, please subscribe our monthly [AI Evaluation Digest][aied] newsletter! If you wanna talk about something I do, feel free to reach out via [email](lexinzhouds@gmail.com) or on [Twitter](https://x.com/lexin_zhou).

[op]: https://www.openphilanthropy.org/
[jhorallo]: https://josephorallo.webs.upv.es/
[avlachos]: https://andreasvlachos.github.io/
[aied]: https://aievaluation.substack.com/
[xxie]: https://scholar.google.com/citations?user=5EQfAFIAAAAJ&hl=en
